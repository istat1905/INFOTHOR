import streamlit as st
import pandas as pd
from scraper import AuchanScraper
import time

# Configuration de la page
st.set_page_config(
    page_title="INFOTHOR - Extracteur Auchan",
    page_icon="ü¶ä",
    layout="wide"
)

# Titre
st.title("ü¶ä INFOTHOR - Extracteur de Commandes Auchan")
st.markdown("---")

# Zone de logs
log_container = st.empty()
logs = []

def add_log(message, status="info"):
    """Ajoute un log avec timestamp"""
    timestamp = time.strftime("%H:%M:%S")
    icon = {
        "info": "‚ÑπÔ∏è",
        "success": "‚úÖ",
        "warning": "‚ö†Ô∏è",
        "error": "‚ùå",
        "loading": "‚è≥"
    }.get(status, "‚ÑπÔ∏è")
    
    logs.append(f"[{timestamp}] {icon} {message}")
    log_container.markdown("\n".join(logs))

# Bouton d'extraction
col1, col2, col3 = st.columns([1, 2, 1])

with col2:
    if st.button("üöÄ EXTRAIRE LES COMMANDES", type="primary", use_container_width=True):
        logs.clear()
        
        try:
            # R√©cup√©ration des credentials depuis secrets
            add_log("Chargement des identifiants...", "loading")
            
            email = st.secrets.get("AUCHAN_EMAIL", "")
            password = st.secrets.get("AUCHAN_PASSWORD", "")
            
            if not email or not password:
                add_log("‚ùå ERREUR : Identifiants manquants dans les secrets Streamlit", "error")
                st.error("‚ö†Ô∏è Configurez AUCHAN_EMAIL et AUCHAN_PASSWORD dans les secrets Streamlit")
                st.stop()
            
            add_log("Identifiants charg√©s avec succ√®s", "success")
            
            # Initialisation du scraper
            add_log("Initialisation du navigateur Firefox...", "loading")
            scraper = AuchanScraper(email, password, headless=True)
            
            add_log("Firefox d√©marr√© avec succ√®s", "success")
            
            # V√©rifier si d√©j√† connect√©
            add_log("V√©rification de la session...", "loading")
            if scraper.is_already_logged_in():
                add_log("‚úÖ D√©j√† connect√© ! Pas besoin de login", "success")
            else:
                add_log("Session expir√©e, connexion n√©cessaire", "info")
                
                # Connexion
                add_log("Navigation vers la page de connexion...", "loading")
                scraper.navigate_to_login()
                add_log("Page de connexion charg√©e", "success")
                
                add_log("Authentification en cours...", "loading")
                scraper.login()
                add_log("Authentification r√©ussie ‚úì", "success")
                
                # Navigation vers commandes
                add_log("Navigation vers la liste des commandes...", "loading")
                scraper.navigate_to_orders()
                add_log("Page des commandes charg√©e", "success")
            
            # R√©initialisation des filtres
            add_log("R√©initialisation des filtres...", "loading")
            scraper.reset_filters()
            add_log("Filtres r√©initialis√©s", "success")
            
            # Configuration de la pagination
            add_log("Configuration : 100 lignes par page...", "loading")
            scraper.set_pagination(100)
            add_log("Pagination configur√©e", "success")
            
            # Tri par date de cr√©ation
            add_log("Tri par date de cr√©ation (d√©croissant)...", "loading")
            scraper.sort_by_creation_date()
            add_log("Tri appliqu√©", "success")
            
            # Extraction des donn√©es
            add_log("Extraction des 20 premi√®res commandes...", "loading")
            data = scraper.extract_orders(limit=20)
            add_log(f"‚úÖ {len(data)} commandes extraites avec succ√®s !", "success")
            
            # Fermeture du navigateur
            scraper.close()
            add_log("Navigateur ferm√©", "info")
            
            # Affichage des r√©sultats
            st.markdown("---")
            st.subheader(f"üìä R√©sultats : {len(data)} commandes")
            
            if data:
                df = pd.DataFrame(data)
                
                # Affichage du tableau
                st.dataframe(
                    df,
                    use_container_width=True,
                    hide_index=True,
                    column_config={
                        "numero": st.column_config.TextColumn("Num√©ro", width="small"),
                        "client": st.column_config.TextColumn("Client", width="medium"),
                        "livrer_a": st.column_config.TextColumn("Livrer √†", width="medium"),
                        "creation_le": st.column_config.TextColumn("Cr√©ation", width="small"),
                        "livrer_le": st.column_config.TextColumn("Livraison", width="small"),
                        "gln": st.column_config.TextColumn("GLN", width="medium"),
                        "montant": st.column_config.NumberColumn("Montant", width="small", format="%.2f ‚Ç¨"),
                        "statut": st.column_config.TextColumn("Statut", width="small")
                    }
                )
                
                # Bouton de t√©l√©chargement CSV
                csv = df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üì• T√©l√©charger en CSV",
                    data=csv,
                    file_name=f"commandes_auchan_{time.strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    use_container_width=True
                )
            else:
                st.warning("Aucune commande trouv√©e")
                
        except Exception as e:
            add_log(f"‚ùå ERREUR : {str(e)}", "error")
            st.error(f"‚ö†Ô∏è Une erreur s'est produite : {str(e)}")
            
            # Tentative de fermeture du navigateur en cas d'erreur
            try:
                if 'scraper' in locals():
                    scraper.close()
            except:
                pass

# Informations dans la sidebar
with st.sidebar:
    st.markdown("### ‚ÑπÔ∏è √Ä propos")
    st.markdown("""
    **INFOTHOR** est un extracteur automatique de commandes depuis la plateforme Auchan.
    
    **Configuration requise :**
    - Firefox install√©
    - Geckodriver install√©
    - Secrets Streamlit configur√©s
    
    **Secrets n√©cessaires :**
    - `AUCHAN_EMAIL`
    - `AUCHAN_PASSWORD`
    """)
    
    st.markdown("---")
    st.markdown("### üîß Fonctionnalit√©s")
    st.markdown("""
    - ‚úÖ Connexion automatique
    - ‚úÖ Reset des filtres
    - ‚úÖ Pagination (100 lignes)
    - ‚úÖ Tri par date
    - ‚úÖ Export CSV
    - ‚úÖ Logs en temps r√©el
    """)
    
    st.markdown("---")
    st.markdown("**Version 1.0** | ü¶ä Firefox + Selenium")
